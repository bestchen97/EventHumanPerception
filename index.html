<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Event-based Human Perception.">
  <meta name="keywords" content="Event, Human Pose Estimation, Human Action Recognition">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Efficient Human Pose Estimation via 3D Event Point Cloud</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>






<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Efficient Human Pose Estimation via 3D Event Point Cloud</h1>
          <h2 class="title is-4">3DV'2022</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Jiaan Chen<sup>1,*</sup>,</span>
	    <span class="author-block">
              Hao Shi<sup>1,*</sup>,
            </span>
            <span class="author-block">
              Yaozu Ye<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://yangkailun.com">Kailun Yang</a><sup>2</sup>,
            </span>
	    <span class="author-block">
              <a href="https://ahupujr.github.io">Lei Sun</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://wangkaiwei.org">Kaiwei Wang</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ZhejiangUniversity,</span>
            <span class="author-block"><sup>2</sup>KIT,</span>
            <span class="author-block"><sup>3</sup>ETH Zurich,</span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2206.04511"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2206.04511"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/MasterHow/EventPointPose"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./figures/paradigm.png"
      class="model"
      alt="paradigm image."/>
      <h2 class="subtitle has-text-centered">
        2D event frame based human pose estimation paradigm vs. the proposed novel 3D event point cloud based paradigm.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">

          <p>
            Human Pose Estimation (HPE) based on RGB images has experienced a rapid development benefiting from deep learning. 
	    However, event-based HPE has not been fully studied, which remains great potential for applications in extreme scenes 
	    and efficiency-critical conditions. In this project, we are the first to estimate 2D human pose directly from 3D event point cloud. 
          </p>
          <p>
	    HPE meets challenges with the drawbacks of frame-based cameras. The prediction of keypoints in scenarios with motion blur 
	    or high dynamic range will be inaccurate. Event cameras, such as the Dynamic Vision Sensor (DVS), a kind of bio-inspired 
	    asynchronous sensor responding to changes in brightness on each pixel independently, with higher dynamic range (over 100dB) and 
	    larger temporal resolution (in the order of us), can tackle these disadvantages, which could maintain stable output in such extreme scenes.
          </p>
          <p>
            We explore the feasibility of estimating human pose from 3D event point clouds directly, which is the first work from this perspective to our best knowledge.
	    We demonstrate the effectiveness of well-known LiDAR point cloud learning backbones for event point cloud based human pose estimation.
	    We propose a new event representation‚Äìrasterized event point cloud, which maintains the 3D features from multiple statistical cues 
	    and significantly reduces memory consumption and computational overhead with the same precision.
          </p>
          <p>
            Our method based on PointNet with 2048 points input achieves 82.46mm in MPJPE3D on the DHP19 dataset, 
	    while only has a latency of 12.29ms on an NVIDIA Jetson Xavier NX edge computing platform, 
	    which is ideally suitable for real-time detection with event cameras.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


    <!-- Modules -->
    <section class="section">
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Modules</h2>
        <div class="hero-body">
          <img src="./figures/module.png"
          class="model"
          alt="model image."/>
          <h2 class="subtitle has-text-justified">
            <p>
              <b>EICA</b> uses a multi-head attention mechanism to fuse event and image features. 
            We transpose the feature maps to reduce the computation complexity from O(h<sup>2</sup>w<sup>2</sup>) to O(c<sup>2</sup>) 
            for high-resolution event-based image deblurring.
            </p>
            <p>
            <b>EMGC</b> selectively connects certain regions of the feature maps of the first stage of our network to the second stage. 
            This is inspired by the observation that regions in which events occur are more severely degraded in the blurry image. 
            We binarize the events and use the resulting "event mask" to guide the restoration.
            </p>
          </h2>
        </div>

        <h2 class="title is-3">Event Representation</h2>
        <div class="columns is-centered">
        <div class="column is-full-width">
          <p style="text-align: center;">
          <img src="./figures/scer.png"
          class="model"
          alt="model image."
          width="50%"/>
         </p>
          <h2 class="subtitle has-text-centered">
          <b>Symmetric Cumulative Event Representation (SCER)</b> feed the asynchronous events to our network, and encodes information about blur in the image by accumulating events at symmetric endpoints.

          </h2>
        </div>
        </div>
      </div>
    </section>
    <!-- Modules -->


    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Qualitative Results</h2>

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <!-- <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div> -->
        <h3 class="title is-4">GoPro</h3>
         <div class="hero-body">
          <h2 class="subtitle has-text-justified">
              <span style="width: 80px;; display:inline-block"> </span> 
              <b>Blurry image</b>
              <span style="width: 140px;; display:inline-block"> </span> 
              <b>Each Channel in SCER</b>  
              <span style="width: 160px;; display:inline-block"> </span>  
              <b>Result</b>             
          </h2>
          <img src="./figures/GOPR0384_11_00.gif"
           class="model"
            alt="model image."/>
         </div>

         <div class="hero-body">
          <img src="./figures/GOPR0410_11_00.gif"
           class="model"
            alt="model image."/>
             <h2 class="subtitle has-text-centered">
             
          </h2>
         </div>

         <div class="hero-body">
          <img src="./figures/GOPR0854_11_00.gif"
           class="model"
            alt="model image."/>
             <h2 class="subtitle has-text-centered">
             
          </h2>
         </div>

         <h3 class="title is-4">REBlur</h3>
         <div class="hero-body">
          <h2 class="subtitle has-text-justified">
            <span style="width: 80px;; display:inline-block"> </span> 
            <b>Blurry image</b>
            <span style="width: 140px;; display:inline-block"> </span> 
            <b>Each Channel in SCER</b>  
            <span style="width: 160px;; display:inline-block"> </span>  
            <b>Result</b>             
        </h2>
          <img src="./figures/camera-fast.gif"
           class="model"
            alt="model image."/>
             <h2 class="subtitle has-text-centered">
             
          </h2>
         </div>
         <div class="hero-body">
          <img src="./figures/checkbox-slow.gif"
           class="model"
            alt="model image."/>
             <h2 class="subtitle has-text-centered">
             
          </h2>
         </div>

      </div>
    </div>


  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{sun2022event,
      author = {Sun, Lei and Sakaridis, Christos and Liang, Jingyun and Jiang, Qi and Yang, Kailun and Sun, Peng and Ye, Yaozu and Wang, Kaiwei and Van Gool, Luc},
      title = {Event-Based Fusion for Motion Deblurring with Cross-modal Attention},
      booktitle = {European Conference on Computer Vision (ECCV)},
      year = 2022
      }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2112.00167">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/ahupujr" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
